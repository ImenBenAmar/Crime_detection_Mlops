import sys
import os
import pickle
import pandas as pd
import numpy as np
import mlflow
from deepchecks.tabular import Dataset
from deepchecks.tabular.suites import model_evaluation
from dotenv import load_dotenv

# Configuration des chemins
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, os.path.abspath(os.path.join(current_dir, '../backend/src')))
ARTIFACTS_PATH = "processors"

# Chargement des variables (DagsHub)
load_dotenv()

def load_data_and_config():
    """Reconstruit des DataFrames pandas √† partir des arrays numpy pour Deepchecks"""
    print("üì¶ Chargement des donn√©es...")
    with open(os.path.join(ARTIFACTS_PATH, "preprocessed_data.pkl"), "rb") as f:
        data = pickle.load(f)
    
    with open(os.path.join(ARTIFACTS_PATH, "features_config.pkl"), "rb") as f:
        config = pickle.load(f)
        
    columns = config['final_feature_order']
    
    # Reconstruction des DataFrames (Deepchecks a besoin de noms de colonnes)
    df_train = pd.DataFrame(data['X_train_scaled'], columns=columns)
    df_train['target'] = data['y_train']
    
    df_test = pd.DataFrame(data['X_test_scaled'], columns=columns)
    df_test['target'] = data['y_test']
    
    return df_train, df_test

def get_best_model():
    """R√©cup√®re le mod√®le champion depuis MLflow"""
    print("üîç R√©cup√©ration du mod√®le depuis MLflow...")
    mlflow.set_tracking_uri(os.getenv("MLFLOW_TRACKING_URI"))
    
    runs = mlflow.search_runs(
        experiment_ids=[mlflow.get_experiment_by_name(os.getenv("DAGSHUB_REPO_NAME", "Crime_MLOPS1")).experiment_id],
        order_by=["metrics.f1_weighted DESC"],
        max_results=1
    )
    
    if runs.empty:
        raise ValueError("Aucun mod√®le trouv√© dans MLflow.")
        
    run_id = runs.iloc[0].run_id
    print(f"üèÜ Meilleur Run ID: {run_id}")
    
    # T√©l√©chargement
    client = mlflow.tracking.MlflowClient()
    local_path = client.download_artifacts(run_id, "model/model.pkl")
    
    with open(local_path, "rb") as f:
        model = pickle.load(f)
        
    return model

def run_quality_check():
    try:
        # 1. Pr√©paration
        df_train, df_test = load_data_and_config()
        model = get_best_model()
        
        # 2. Cr√©ation des Datasets Deepchecks
        # cat_features=[] car les donn√©es sont d√©j√† encod√©es/scal√©es
        ds_train = Dataset(df_train, label='target', cat_features=[])
        ds_test = Dataset(df_test, label='target', cat_features=[])
        
        # 3. Lancement de la Suite "Model Evaluation"
        # V√©rifie : Performance, Overfitting, Biais, etc.
        print("üöÄ Lancement de l'analyse Deepchecks (cela peut prendre un moment)...")
        suite = model_evaluation()
        result = suite.run(train_dataset=ds_train, test_dataset=ds_test, model=model)
        
        # 4. Sauvegarde du rapport
        report_file = "model_quality_report.html"
        result.save_as_html(report_file)
        print(f"‚úÖ Rapport g√©n√©r√© : {report_file}")
        
        # 5. V√©rification des conditions (Quality Gate)
        # On peut √™tre strict (result.passed()) ou permissif (juste g√©n√©rer le rapport)
        # Ici, on affiche juste les r√©sultats en JSON pour les logs Jenkins
        print(result.to_json())
        
        # Pour bloquer le pipeline en cas d'√©chec critique, d√©commentez ceci :
        # if not result.passed():
        #     print("‚ùå La qualit√© du mod√®le est insuffisante !")
        #     sys.exit(1)
            
    except Exception as e:
        print(f"‚ùå Erreur lors du test de qualit√© : {e}")
        sys.exit(1)

if __name__ == "__main__":
    run_quality_check()