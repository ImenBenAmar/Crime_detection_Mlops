{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84560ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è Project ID utilis√© (nettoy√©) : 01f16019b2950-f65e-7167-a3f4-67232238fbfd\n",
      "üìÇ Chargement des donn√©es...\n",
      "   ‚úÖ Reference: (319110, 18)\n",
      "   ‚úÖ Current: (79778, 18)\n",
      "‚òÅÔ∏è Connexion √† Evidently Cloud...\n",
      "üìä G√©n√©ration du rapport...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "# On ignore les warnings \"invalid value encountered in divide\" qui polluent tes logs\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import DataDriftPreset, DataQualityPreset\n",
    "from evidently.test_suite import TestSuite\n",
    "from evidently.tests import TestNumberOfColumnsWithMissingValues, TestShareOfDriftedColumns\n",
    "from evidently.ui.workspace.remote import RemoteWorkspace\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION & NETTOYAGE (LE FIX EST ICI)\n",
    "# ==========================================\n",
    "EVIDENTLY_TOKEN = os.environ.get(\"EVIDENTLY_TOKEN\")\n",
    "raw_project_id = os.environ.get(\"EVIDENTLY_PROJECT_ID\", \"\")\n",
    "\n",
    "# üßπ NETTOYAGE : On enl√®ve les guillemets (\" ou ') et les espaces vides\n",
    "EVIDENTLY_PROJECT_ID = str(raw_project_id).strip().replace('\"', '').replace(\"'\", \"\")\n",
    "\n",
    "print(f\"‚ÑπÔ∏è Project ID utilis√© (nettoy√©) : {EVIDENTLY_PROJECT_ID}\")\n",
    "\n",
    "ARTIFACTS_PATH = \"../backend/src/processors\" \n",
    "\n",
    "if not EVIDENTLY_TOKEN or not EVIDENTLY_PROJECT_ID:\n",
    "    print(\"‚ùå ERREUR : Token ou Project ID manquant.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ==========================================\n",
    "# 2. CHARGEMENT DES DONN√âES\n",
    "# ==========================================\n",
    "print(\"üìÇ Chargement des donn√©es...\")\n",
    "\n",
    "try:\n",
    "    # Adapte ce chemin si n√©cessaire (ex: ../../backend/src/processors)\n",
    "    # selon o√π tu lances le script\n",
    "    with open(os.path.join(ARTIFACTS_PATH, \"preprocessed_data.pkl\"), \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    with open(os.path.join(ARTIFACTS_PATH, \"features_config.pkl\"), \"rb\") as f:\n",
    "        config = pickle.load(f)\n",
    "    \n",
    "    columns = config['final_feature_order']\n",
    "    \n",
    "    reference_data = pd.DataFrame(data['X_train_scaled'], columns=columns)\n",
    "    reference_data['target'] = data['y_train'] \n",
    "    \n",
    "    current_data = pd.DataFrame(data['X_test_scaled'], columns=columns)\n",
    "    current_data['target'] = data['y_test']\n",
    "    \n",
    "    print(f\"   ‚úÖ Reference: {reference_data.shape}\")\n",
    "    print(f\"   ‚úÖ Current: {current_data.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur chargement donn√©es : {e}\")\n",
    "    # En local pour tester, tu peux commenter sys.exit(1) si tu veux juste tester la connexion cloud\n",
    "    sys.exit(1)\n",
    "\n",
    "# ==========================================\n",
    "# 3. DASHBOARD CLOUD\n",
    "# ==========================================\n",
    "print(\"‚òÅÔ∏è Connexion √† Evidently Cloud...\")\n",
    "\n",
    "try:\n",
    "    ws = RemoteWorkspace(\"https://app.evidently.cloud\", EVIDENTLY_TOKEN)\n",
    "    \n",
    "    # On v√©rifie que l'ID est valide pour Evidently\n",
    "    # ws.search_project(EVIDENTLY_PROJECT_ID) # Optionnel\n",
    "    \n",
    "    print(\"üìä G√©n√©ration du rapport...\")\n",
    "    report = Report(metrics=[\n",
    "        DataDriftPreset(),\n",
    "        DataQualityPreset()\n",
    "    ])\n",
    "    \n",
    "    report.run(reference_data=reference_data, current_data=current_data)\n",
    "    \n",
    "    print(\"‚¨ÜÔ∏è Envoi vers le Cloud...\")\n",
    "    ws.add_report(EVIDENTLY_PROJECT_ID, report)\n",
    "    print(\"‚úÖ Rapport envoy√© avec succ√®s !\")\n",
    "\n",
    "except ValueError as ve:\n",
    "    print(f\"‚ùå ERREUR UUID : L'ID du projet est toujours invalide : {EVIDENTLY_PROJECT_ID}\")\n",
    "    print(\"üëâ V√©rifie ta variable d'environnement, elle ne doit contenir que des chiffres et des lettres (a-f).\")\n",
    "    print(f\"D√©tail : {ve}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erreur Cloud (Autre) : {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. QUALITY GATE\n",
    "# ==========================================\n",
    "print(\"üõ°Ô∏è Ex√©cution des Tests...\")\n",
    "\n",
    "tests = TestSuite(tests=[\n",
    "    TestShareOfDriftedColumns(lt=0.5), \n",
    "    TestNumberOfColumnsWithMissingValues(eq=0) \n",
    "])\n",
    "\n",
    "tests.run(reference_data=reference_data, current_data=current_data)\n",
    "test_results = tests.as_dict()\n",
    "\n",
    "if not test_results['summary']['all_passed']:\n",
    "    print(\"‚ùå ECHEC : Data Drift critique !\")\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    print(\"‚úÖ SUCC√àS : Donn√©es stables.\")\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e288e851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Feature Store: Artifacts loaded.\n",
      "üì¶ Chargement des donn√©es de r√©f√©rence (Train)...\n",
      "‚ö†Ô∏è Fichier de nouvelles donn√©es introuvable : c:\\Users\\asus\\Desktop\\mlops\\MLOPS\\data\\last_rows.csv\n",
      "   -> Cr√©ation d'un dataset factice pour le test...\n",
      "üìä Comparaison : Ref (319110, 17) vs New (100, 17)\n",
      "üöÄ Analyse du Drift en cours...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\Desktop\\mlops\\MLOPS\\venv\\lib\\site-packages\\numpy\\lib\\_function_base_impl.py:2922: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in divide\n",
      "\n",
      "c:\\Users\\asus\\Desktop\\mlops\\MLOPS\\venv\\lib\\site-packages\\numpy\\lib\\_function_base_impl.py:2923: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in divide\n",
      "\n",
      "c:\\Users\\asus\\Desktop\\mlops\\MLOPS\\venv\\lib\\site-packages\\numpy\\lib\\_function_base_impl.py:2922: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in divide\n",
      "\n",
      "c:\\Users\\asus\\Desktop\\mlops\\MLOPS\\venv\\lib\\site-packages\\numpy\\lib\\_function_base_impl.py:2923: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Rapport g√©n√©r√© : c:\\Users\\asus\\Desktop\\mlops\\MLOPS\\monitoring\\production_drift_report.html\n",
      "‚úÖ Tout va bien. Pas de d√©rive significative.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\Desktop\\mlops\\MLOPS\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3558: UserWarning:\n",
      "\n",
      "To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import DataDriftPreset\n",
    "from evidently.test_suite import TestSuite\n",
    "from evidently.tests import TestShareOfDriftedColumns\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION ET CHEMINS\n",
    "# ==========================================\n",
    "current_dir = os.getcwd()  # R√©pertoire courant\n",
    "\n",
    "# Chemin vers backend/src pour importer feature_store.py\n",
    "BACKEND_SRC_PATH = os.path.abspath(os.path.join(current_dir, '..', 'backend', 'src'))\n",
    "sys.path.insert(0, BACKEND_SRC_PATH)\n",
    "\n",
    "# Chemin vers les processeurs (.pkl)\n",
    "ARTIFACTS_PATH = os.path.abspath(os.path.join(BACKEND_SRC_PATH, 'processors'))\n",
    "\n",
    "# Chemin vers les nouvelles donn√©es brutes (Simulation de prod)\n",
    "# On suppose que le fichier est dans data/crime_v1.csv √† la racine du projet\n",
    "NEW_DATA_PATH = os.path.abspath(os.path.join(current_dir,'..', 'data', 'last_rows.csv'))\n",
    "\n",
    "# Import du Feature Store (La m√™me logique que l'API !)\n",
    "try:\n",
    "    from feature_store import CrimeFeatureStore\n",
    "except ImportError:\n",
    "    print(\"‚ùå Impossible d'importer CrimeFeatureStore. V√©rifiez que backend/src/feature_store.py existe.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "def get_reference_data():\n",
    "    \"\"\"Charge les donn√©es d'entra√Ænement (Reference) depuis le pickle\"\"\"\n",
    "    print(\"üì¶ Chargement des donn√©es de r√©f√©rence (Train)...\")\n",
    "    data_path = os.path.join(ARTIFACTS_PATH, \"preprocessed_data.pkl\")\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"‚ùå Fichier manquant : {data_path}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    with open(data_path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    # On a besoin des noms de colonnes pour Evidently\n",
    "    # On peut les r√©cup√©rer via le Feature Store ou le config pickle\n",
    "    with open(os.path.join(ARTIFACTS_PATH, \"features_config.pkl\"), \"rb\") as f:\n",
    "        config = pickle.load(f)\n",
    "        \n",
    "    cols = config['final_feature_order']\n",
    "    \n",
    "    # Reconstruction du DataFrame Reference\n",
    "    ref_df = pd.DataFrame(data['X_train_scaled'], columns=cols)\n",
    "    # Optionnel : Ajouter la target si on veut monitorer le Concept Drift\n",
    "    # ref_df['target'] = data['y_train'] \n",
    "    \n",
    "    return ref_df\n",
    "\n",
    "def process_current_data_with_store(df_raw, store):\n",
    "    \"\"\"\n",
    "    Transforme les nouvelles donn√©es brutes en utilisant le Feature Store.\n",
    "    Simule ce qui se passe dans l'API.\n",
    "    \"\"\"\n",
    "    print(\"‚öôÔ∏è Transformation des nouvelles donn√©es via Feature Store...\")\n",
    "    \n",
    "    processed_rows = []\n",
    "    \n",
    "    # On it√®re sur chaque ligne comme si c'√©tait une requ√™te API unique\n",
    "    # C'est un peu plus lent que du batch, mais √ßa garantit 100% de coh√©rence avec l'API\n",
    "    input_records = df_raw.to_dict(orient='records')\n",
    "    \n",
    "    for record in input_records:\n",
    "        # get_online_features renvoie un tableau numpy (1, N)\n",
    "        vector = store.get_online_features(record)\n",
    "        processed_rows.append(vector[0]) # On prend la premi√®re ligne (le vecteur plat)\n",
    "    \n",
    "    # On recr√©e un DataFrame avec les bonnes colonnes\n",
    "    current_df = pd.DataFrame(processed_rows, columns=store.required_features)\n",
    "    \n",
    "    return current_df\n",
    "\n",
    "def run_production_monitoring():\n",
    "    try:\n",
    "        # 1. Initialisation du Feature Store\n",
    "        store = CrimeFeatureStore(processors_path=ARTIFACTS_PATH)\n",
    "        store.load_artifacts()\n",
    "        \n",
    "        # 2. R√©cup√©ration de la R√©f√©rence\n",
    "        reference_data = get_reference_data()\n",
    "        \n",
    "        # 3. R√©cup√©ration et Traitement du Current (Nouveau)\n",
    "        if not os.path.exists(NEW_DATA_PATH):\n",
    "            print(f\"‚ö†Ô∏è Fichier de nouvelles donn√©es introuvable : {NEW_DATA_PATH}\")\n",
    "            print(\"   -> Cr√©ation d'un dataset factice pour le test...\")\n",
    "            # Si pas de fichier, on prend un √©chantillon du train pour tester le script\n",
    "            current_data = reference_data.sample(100)\n",
    "        else:\n",
    "            print(f\"üìÇ Lecture des nouvelles donn√©es : {NEW_DATA_PATH}\")\n",
    "            # On prend un √©chantillon al√©atoire de 500 lignes\n",
    "            df_new_raw = pd.read_csv(NEW_DATA_PATH).sample(500)\n",
    "            \n",
    "            # Transformation via le Store\n",
    "            current_data = process_current_data_with_store(df_new_raw, store)\n",
    "        \n",
    "        print(f\"üìä Comparaison : Ref {reference_data.shape} vs New {current_data.shape}\")\n",
    "\n",
    "        # 4. Analyse du Drift avec Evidently\n",
    "        print(\"üöÄ Analyse du Drift en cours...\")\n",
    "        \n",
    "        drift_suite = TestSuite(tests=[\n",
    "            TestShareOfDriftedColumns(lt=0.3) # Alerte si > 30% des colonnes d√©vient\n",
    "        ])\n",
    "        \n",
    "        drift_suite.run(reference_data=reference_data, current_data=current_data)\n",
    "        \n",
    "        # Sauvegarde\n",
    "        report_path = \"production_drift_report.html\"\n",
    "        drift_suite.save_html(report_path)\n",
    "        print(f\"‚úÖ Rapport g√©n√©r√© : {os.path.abspath(report_path)}\")\n",
    "        \n",
    "        # 5. Verdict\n",
    "        results = drift_suite.as_dict()\n",
    "        if not results['summary']['all_passed']:\n",
    "            print(\"üö® ALERTE : Le mod√®le d√©rive ! (Data Drift detected)\")\n",
    "            sys.exit(1)\n",
    "        else:\n",
    "            print(\"‚úÖ Tout va bien. Pas de d√©rive significative.\")\n",
    "            sys.exit(0)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur critique : {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_production_monitoring()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd81eb12",
   "metadata": {},
   "source": [
    "Pourquoi cela a-t-il √©chou√© ?\n",
    "C'est tout √† fait normal dans votre contexte de test :\n",
    "Vous avez utilis√© un petit √©chantillon (probablement 100 ou 500 lignes).\n",
    "Sur un petit √©chantillon, les statistiques bougent beaucoup par rapport au gros fichier d'entra√Ænement (300k lignes).\n",
    "Evidently a d√©tect√© ces diff√©rences statistiques l√©gitimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08cd3bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Les 300 derni√®res lignes ont √©t√© enregistr√©es dans C:\\Users\\asus\\Desktop\\mlops\\data\\crime_last5.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Chemin du fichier source\n",
    "input_file = r\"C:\\Users\\asus\\Desktop\\mlops\\data\\data.csv\"\n",
    "\n",
    "# Charger le CSV\n",
    "data = pd.read_csv(input_file)\n",
    "\n",
    "# Prendre les 5 derni√®res lignes\n",
    "last_rows1 = data.tail(400)\n",
    "\n",
    "# Chemin du fichier de sortie\n",
    "output_file = r\"C:\\Users\\asus\\Desktop\\mlops\\data\\crime_last5.csv\"\n",
    "\n",
    "# Sauvegarder dans un nouveau CSV\n",
    "last_rows1.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"‚úÖ Les 300 derni√®res lignes ont √©t√© enregistr√©es dans {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f71824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
